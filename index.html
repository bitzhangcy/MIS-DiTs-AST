<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Academic Project Page</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Enhancing Multi-Instance Synthesis in Diffusion Transformers with Attention Speciality Tuning</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="FIRST AUTHOR PERSONAL LINK" target="_blank">Chunyang Zhang</a><sup>1*</sup>,</span>
                <span class="author-block">
                  <a href="SECOND AUTHOR PERSONAL LINK" target="_blank">Zhenhong Sun</a><sup>2*</sup>,</span>
                  <span class="author-block">
                    <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Zhicheng Zhang</a><sup>3</sup>,</span>
                  <span class="author-block">
                    <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Junyan Wang</a><sup>4</sup>,</span>
                  <span class="author-block">
                    <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Yu Zhang</a><sup>3</sup>,</span>
                  <span class="author-block">
                    <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Dong Gong</a><sup>4</sup>,</span>
                  <span class="author-block">
                    <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Huadong Mo</a><sup>5✉</sup>,</span>
                  <span class="author-block">
                    <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Daoyi Dong</a><sup>6✉</sup>,</span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><small><br><sup>1</sup>School of System and Computing, University of New South Wales</small></span>
                    <span class="author-block"><small><br><sup>2</sup>School of Engineering, Australian National University</small></span>
                    <span class="author-block"><small><br><sup>3</sup>School of Business, University of New South Wales</small></span>
                    <span class="author-block"><small><br><sup>4</sup>Australian Institute for Machine Learning, University of Adelaide</small></span>
                    <span class="author-block"><small><br><sup>5</sup>School of Computer Science and Engineering, University of New South Wales</small></span>
                    <span class="author-block"><small><br><sup>6</sup>School of Computer Science, University of Technology Sydney</small></span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Equal Contribution</small></span>
                    <span class="eql-cntrb"><small><br><sup>✉</sup>Corresponding Authors</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link
                    <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span>
                  -->
                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/bitzhangcy/MIS_DiT" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        <source src="static/videos/banner_video.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        Aliquam vitae elit ullamcorper tellus egestas pellentesque. Ut lacus tellus, maximus vel lectus at, placerat pretium mi. Maecenas dignissim tincidunt vestibulum. Sed consequat hendrerit nisl ut maximus. 
      </h2>
    </div>
  </div>
</section>
-->
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Text-to-image (T2I) generation models often struggle with multi-instance synthesis (MIS), where they must accurately depict multiple distinct instances in a single image based on complex prompts detailing individual features. Traditional MIS control methods for UNet architectures like SD v1.5/SDXL fail to adapt to DiT-based models like FLUX and SD v3.5, which rely on integrated attention between image and text tokens rather than text-image cross-attention. To enhance MIS in DiT, we first analyze the mixed attention mechanism in DiT. Our token-wise and layer-wise analysis of attention maps reveals a hierarchical response structure: instance tokens dominate early layers, background tokens in middle layers, and attribute tokens in later layers. Building on this observation, we propose a training-free approach for enhancing MIS in DiT-based models with hierarchical and step-layer-wise attention specialty tuning (AST). AST amplifies key regions while suppressing irrelevant areas in distinct attention maps across layers and steps, guided by the hierarchical structure. This optimizes multimodal interactions by hierarchically decoupling the complex prompts with instance-based sketches. We evaluate our approach using upgraded sketch-based layouts for the T2I-CompBench and customized complex scenes.
            Both quantitative and qualitative results confirm our method enhances complex layout generation, ensuring precise instance placement and attribute representation in MIS.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Hierarchical Attention Responses of DiT Model</h2>
        <div class="columns is-centered">
          <div class="column is-full-width">
            <img src="./static/images/FLUX.png" class="method-image" alt="Method illustration" />
            <p>
              Schematic architecture of the DiT-based models (a) with double/single-stream attentions (b).  The attention map in (c) reveals four distinct interaction patterns: text-to-text, text-to-image, image-to-text, and image-to-image, enabling deep consistent fusion between text and visual tokens.
            </p>
          </div>
        </div>
        <div class="columns is-centered">
          <div class="column is-full-width">
            <img src="./static/images/attn.png" class="method-image" alt="Method illustration" />
            <p>
              Attention map averages for the prompt ``Red cube in a forest".
              (a) T2T maps show strong intra-segment interactions within valid tokens of ``Red cube" (first 3 tokens) and ``in a forest" (last 4 tokens).
              (b) I2I maps (the maximum value of each point relative to all other points) reveal growing interaction between ``Red cube" and ``a forest" segments over steps.
              (c/d/e)  I2T attention maps on individual tokens highlight that instance tokens dominate early layers, background tokens in the middle, and color tokens later, with most information integrated in the first half of steps. T2I maps showing lower scores with weak impact and thus are neglected.
            </p>
          </div>
        </div>

        <div class="columns is-centered">
          <div class="column is-full-width">
            <img src="./static/images/analysis.png" class="method-image" alt="Method illustration" />
            <p>
              Step-Layer-Wise Token Exchange involved injecting specific tokens between two prompts formatted as ``[color] [Instance] in [Background]" (a). First 6-step exchanging results show that specific token concept can be replaced in certain layers (e.g., L25-27) with other concepts preserving, aligning with the hierarchical attention responses.
            </p>
          </div>
        </div>

      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Methods</h2>
        <div class="columns is-centered">
          <div class="column is-full-width">
            <img src="./static/images/method.png" class="method-image" alt="Method illustration" />
            <p>
              Overview of the hierarchical tuning strategy for DiT models. (a) Attention specialty tuning enhances meaningful regions while suppressing non-meaningful ones via a unified scalable module with distinct tuning masks.
              (b) Tuning masks for T2T and I2I masks are directly built from prompt segments and sketches.
              (c) Hierarchical and step-layer-wise module refines alignment by adjusting each component’s impact across specific layers and steps, assigning layers for attributes, instances, and background tokens within first 16 steps.
            </p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Qualitative Comparisons on Multi-Instance Synthesis</h2>
        <div class="columns is-centered">
          <div class="column is-full-width">
            <img src="./static/images/comparison.png" class="method-image" alt="Method illustration" />
            <p>
              Our approach extends DiT models (SD v3.5 and FLUX) with better multi-instance alignment with prompts and sketch-based layouts.
            </p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Complex Layout Application</h2>
          <div class="columns is-centered">
            <div class="column is-full-width">
              <img src="./static/images/ccases.png" class="method-image" alt="Method illustration" />
              <p>
                Our method demonstrates precision and adaptability in managing complex scenarios, as shown by the visualization of handling small/repeated instances
              </p>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Ablation Study</h2>
          <div class="columns is-centered">
            <div class="column is-full-width">
              <img src="./static/images/ablation.png" class="method-image" alt="Method illustration" />
              <p>
                Ablation study on attention modules. The I2T module mainly enhances layout-prompt alignment, T2T improves token interactions within segments, and I2I further refines instance self-alignment. Arrow direction indicates better.
              </p>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{,
  title={Enhancing Multi-Instance Synthesis in Diffusion Transformers with Attention Speciality Tuning},
  author={},
  journal={arXiv preprint arXiv:2503.},
  year={2025}
}</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p style="text-align: center">
            Source code of this page can be found at
            <a
                    href="https://github.com/bitzhangcy/MIS-DiTs-AST"
                    target="_blank"
            >Personalize-Anything-Page</a
            >. The website template is forked from
            <a
                    href="https://github.com/eliahuhorwitz/Academic-project-page-template"
                    target="_blank"
            >MV-Adapter-Page</a
            >.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
