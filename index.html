<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Attention Specialty for Diffusion Transformers</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Hierarchical and Step-Layer-Wise Tuning of Attention Specialty for Multi-Instance Synthesis in Diffusion Transformers</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=0hXn5p8AAAAJ" target="_blank">Chunyang Zhang</a><sup>1*</sup>,</span>
                <span class="author-block">
                  <a href="https://scholar.google.com.hk/citations?user=eDiXHP8AAAAJ&hl=zh-CN" target="_blank">Zhenhong Sun</a><sup>2*†</sup>,</span>
                  <span class="author-block">
                    <a href="" target="_blank">Zhicheng Zhang</a><sup>1</sup>,</span>
                  <span class="author-block">
                    <a href="https://scholar.google.com.au/citations?hl=en&user=5yS_tTUAAAAJ" target="_blank">Junyan Wang</a><sup>3</sup>,</span>
                  <span class="author-block">
                    <a href="https://scholar.google.com.au/citations?user=wFVyO90AAAAJ&hl=en" target="_blank">Yu Zhang</a><sup>1</sup>,</span>
                  <span class="author-block">
                    <a href="https://donggong1.github.io/" target="_blank">Dong Gong</a><sup>1</sup>,</span>
                  <span class="author-block">
                    <a href="https://scholar.google.com/citations?user=-s1nb1wAAAAJ&hl=en" target="_blank">Huadong Mo</a><sup>1✉</sup>,</span>
                  <span class="author-block">
                    <a href="https://scholar.google.com.au/citations?user=ACw1Y4oAAAAJ&hl=en" target="_blank">Daoyi Dong</a><sup>2,4✉</sup>,</span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block" style="display: inline-block;"><small><br><sup>1</sup>University of New South Wales</small></span>
                    <span class="author-block" style="display: inline-block;"><small><br><sup>2</sup>Australian National University</small></span>
                    <span class="author-block" style="display: inline-block;"><small><br><sup>3</sup>University of Adelaide</small></span>
                    <span class="author-block" style="display: inline-block;"><small><br><sup>4</sup>University of Technology Sydney</small></span>
                    <span class="eql-cntrb" style="display: inline-block;"><small><br><sup>*</sup>Equal Contribution</small></span>
                    <span class="eql-cntrb" style="display: inline-block;"><small><br><sup>†</sup>Project Lead</small></span>
                    <span class="eql-cntrb" style="display: inline-block;"><small><br><sup>✉</sup>Corresponding Authors</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/2504.10148.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link
                    <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span>
                  -->
                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/EngineeringAI-LAB/MIS-DiT-AST" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2504.10148" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        <source src="static/videos/banner_video.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        Aliquam vitae elit ullamcorper tellus egestas pellentesque. Ut lacus tellus, maximus vel lectus at, placerat pretium mi. Maecenas dignissim tincidunt vestibulum. Sed consequat hendrerit nisl ut maximus. 
      </h2>
    </div>
  </div>
</section>
-->
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Text-to-image (T2I) generation models often struggle with multi-instance synthesis (MIS), where they must accurately depict multiple distinct instances in a single image based on complex prompts detailing individual features. Traditional MIS control methods for UNet architectures like SD v1.5/SDXL fail to adapt to DiT-based models like FLUX and SD v3.5, which rely on integrated attention between image and text tokens rather than text-image cross-attention. To enhance MIS in DiT, we first analyze the mixed attention mechanism in DiT. Our token-wise and layer-wise analysis of attention maps reveals a hierarchical response structure: instance tokens dominate early layers, background tokens in middle layers, and attribute tokens in later layers. Building on this observation, we propose a training-free approach for enhancing MIS in DiT-based models with hierarchical and step-layer-wise attention specialty tuning (AST). AST amplifies key regions while suppressing irrelevant areas in distinct attention maps across layers and steps, guided by the hierarchical structure. This optimizes multimodal interactions by hierarchically decoupling the complex prompts with instance-based sketches. We evaluate our approach using upgraded sketch-based layouts for the T2I-CompBench and customized complex scenes.
            Both quantitative and qualitative results confirm our method enhances complex layout generation, ensuring precise instance placement and attribute representation in MIS.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Motivations</h2>
        <div class="columns is-centered">
          <div class="column is-full-width">
            <img src="./static/images/FLUX.png" class="method-image" alt="Method illustration" />
            <p>
              <strong>Architecture Overview of DiT-based Models:</strong> DiT‑based models is illustrated in (a), with (b) contrasting double‑stream and single‑stream attention configurations. The attention map in (c) reveals four distinct interaction modalities—text‑to‑text, text‑to‑image, image‑to‑text, and image‑to‑image—enabling deep, coherent semantic fusion between textual and visual tokens.
            </p>
          </div>
        </div>
        <div class="columns is-centered">
          <div class="column is-full-width">
            <img src="./static/images/attn.png" class="method-image" alt="Method illustration" />
            <p>
              <strong>Attention Dynamics for "Red cube in a forest":</strong> Our analysis reveals distinct attention patterns across modalities: (1) T2T maps establish strong intra-segment binding during early processing, (2) I2I interactions gradually emerge between object and environment, while (3) I2T attention progresses through staged feature integration. T2I maps show negligible impact and are excluded from consideration.
            </p>
          </div>
        </div>

        <div class="columns is-centered">
          <div class="column is-full-width">
            <img src="./static/images/analysis.png" class="method-image" alt="Method illustration" />
            <p>
              <strong>Token Exchange Dynamics Analysis:</strong> First 6-step exchanging results show that specific token concept can be replaced in certain layers (e.g., L25-27) with other concepts preserving, aligning with the hierarchical attention responses.
            </p>
          </div>
        </div>

      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Method</h2>
        <div class="columns is-centered">
          <div class="column is-full-width">
            <img src="./static/images/method.png" class="method-image" alt="Method illustration" />
            <p>
              <strong>Hierarchical Tuning Strategy for DiT-based Models:</strong> Our approach anchors semantic precision in early denoising through mask-guided attention modulation with preserved structural encoding, and transitions to hierarchical attention refinement for layout-text alignment in later steps. During attention modulation, we inject spatial guidance via sketch-based mask perturbations. This layer-adaptive strategy balances layout fidelity and generative diversity.
            </p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Experiments</h2>
        <div class="columns is-centered">
          <div class="column is-full-width">
            <img src="./static/images/comparison.png" class="method-image" alt="Method illustration" />
            <p>
              <strong>Qualitative Comparisons on Multi-Instance Synthesis:</strong> Our approach extends DiT-based models (SD v3.5 and FLUX) with better multi-instance alignment with prompts and sketch-based layouts.
            </p>
          </div>
        </div>
        <div class="columns is-centered">
          <!-- 第一列 -->
          <div class="column is-half">
            <img src="./static/images/ccases.png" class="method-image" alt="Complex cases illustration" />
            <p class="has-text-centered">
              <strong>More Complex Applications:</strong> Our method demonstrates precision and adaptability in managing complex scenarios, as shown by the visualization of handling small/repeated instances
            </p>
          </div>

          <!-- 第二列 -->
          <div class="column is-half">
            <img src="./static/images/ablation.png" class="method-image" alt="Ablation study" />
            <p class="has-text-centered">
              <strong>Ablation study:</strong> The I2T module mainly enhances layout-prompt alignment, T2T improves token interactions within segments, and I2I further refines instance self-alignment. Arrow direction indicates better.
            </p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{,
  title={Hierarchical and Step-Layer-Wise Tuning of Attention Specialty for Multi-Instance Synthesis in Diffusion Transformers},
  author={Zhang, Chunyang and Sun, Zhenhong and Zhang, Zhicheng and Wang, Junyan and Zhang, Yu and Gong, Dong and Mo, Huadong and Dong, Daoyi},
  journal={arXiv preprint arXiv:2504.10148},
  year={2025}
}</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p style="text-align: center">
            Source code of this page can be found at
            <a
                    href="https://github.com/bitzhangcy/MIS-DiTs-AST"
                    target="_blank"
            >MIS-DiTs-AST</a
            >. The website template is forked from
            <a
                    href="https://github.com/eliahuhorwitz/Academic-project-page-template"
                    target="_blank"
            >Academic-project-page-template</a
            >.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
